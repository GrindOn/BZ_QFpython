# 爬虫第六天

# 一、回顾上周知识点

### 1.1 爬虫的认知

```
- 数据请求(网络请求库)
- 数据解析(re/xpath/bs4)
- 数据存储(csv/pymysql/json??)
- 反反爬的策略
	- ip代理 
	- ua池
	- cookie池： 收集手动登录之后的响应的Cookie信息
	- 请求间隔(2~5秒)
	- 验证码处理(打码平台、机器学习？？？)
```

### 1.2 网络请求库

```
- urllib
	- request
		- urlopen()
		- urlretrieve(fullurl, filename)
		- Request(url, data=None, headers)
		- build_opener(*handlers)
		- HTTPHandler
		- HTTPCookieProcessor(http.cookiejar.CookieJar())
		- ProxyHandler(proxies={})
	- parse
		- quote()
		- urlencode()
- http.client.HTTPResponse
	- code
	- getheaders()
	- getheader(name, default)
	- read()  读取的响应字节数据
	
		
- requests (第三方)
	- request(method, url, params, data, json, files, headers, cookies, proxies, auth)
	- get(url, params, **kwargs)
	- post(url, data, json, **kwargs)
	- put(url, data, json, **kwargs)
	- delete(url, **kwargs)
	- Response
		- status_code
		- encoding
		- headers
		- content 字节数据
		- text 文本数据
		- json()  json文本反序列化为Python的dict/list的对象
```

### 1.3 数据解析

```
- re
- xpath  (pip install lxml)
	- from lxml import etree
	  root = etree.HTML(html)
	  root.xpath('')  # list[''] / list[<Element>, ]
  - 返回文本列表的xpath表示
  	- @href/@src 标签属性
  	- text() 标签文本
  - 返回Element元素列表
  	- //title
  	- //ul/li[1]
  	
- bs4 (pip install bs4)
	- from bs4 import BeautifulSoup
	  root = BeautifulSoup(html, 'lxml')  # bs4.element.Tag  
	- 查询元素标签的方法
		- find('标签名', class_, id_) 查找第一个
		- find_all('标签名', class_, id_, limit=N) 查找前N个
		- select('css选择器')
			- #id
			- .classname
			- 标签名
			- 后代标签
			- 兄弟标签 (查找多个标签)
			- 属性标签
			- 伪类
			
	- Tag属性
		- string/text
		- get_text()
		- attrs: dict 标签中所有属性的字典
		- contents 子标签的文本列表
		- descendants 子标签的Tag列表
```

### 1.4 多任务爬虫

 - 多线程
    - threading
      	- Thread
   - queue.Queue  线程队列
 - 多进程
   - multiprocessing
     - Process
     - Queue  进程队列
- 协程
  - asyncio
    - coroutine 协程装饰器
    - get_event_loop()
    - wait()
    - sleep()
  - yield from 
  - async / await

### 1.5 selenium框架

```
以driver程序驱动浏览器，对目标(网站或网页)进行操作(请求网页、提取数据、截图、切换或关闭页签-window)。
```

```
- chrome.get() 打开目标(发起请求)
- chrome.quit() 退出浏览器
- chrome.close() 关闭当前的窗口
- chrome.find_element(By, value)
	- selenium.webdriver.common.by.By
		- ID
		- CLASS_NAME
		- NAME
		- XPATH
		- CSS_SELECTOR
		_ LINK_TEXT
	- WebElement 查到的标签对象
		- get_attribute('属性名', default)
		- text 标签文本
		- click()
		- send_keys()
		- rect 当前元素的位置(left, top, width, height)
		
- chrome.find_elements(By, value)
- execute_script()
- save_screenshot(filename)  截图
- 等待某一个标签元素出现
	- selenium.webdriver.support
		- ui
			- WebDriverWait
		- expected_conditions
			- visibility_of_all_elements_located((By, value))
		
	ui.WebDriverWait(dirver, timeout).until(
			expected_conditions, 
			error_msg
	)
```

### 1.6 docker

```
容器技术，将远程的docker仓库中的镜像下拉到本地， 再将镜像运行成为一个容器(进程)。
```

```
- 镜像操作
	- 基本信息 
		- 名称
		- 版本
		- ID
		- 描述
	- docker images 查看所有镜像
	- docker rmi 名称:版本号 / ID 删除镜像
	- docker run 名称:版本号 / ID 启动镜像
		- -dit 后台启动镜像，启动后可进入容器并打开新的terminal(终端)
		- -p 宿主机端口: 容器端口
- 容器操作
	- docker ps 查看正运行的容器
		- -a 查看所有的容器
		- -l 查看最后一个启动的容器
		
	- docker logs 容器名或ID  查看容器运行的日志
	- docker exec 容器名或ID Linux命令 在容器中执行Linux命令
		- docker exec -it 容器名或ID bash 进入容器
	- docker stop 容器名或ID
	- docker start 容器名或ID
	- docker restart 容器名或ID
	- docker rm -f 容器名或ID 删除容器， -f强制删除正运行的容器
```

## 二、日志模块进阶

### 2.1 日志格式

| 格式           | 说明                               |
| -------------- | ---------------------------------- |
| %(name)s       | 记录器的名称, 默认为root           |
| %(levelno)s    | 数字形式的日志记录级别             |
| %(levelname)s  | 日志记录级别的文本名称             |
| %(filename)s   | 执行日志记录调用的源文件的文件名称 |
| %(pathname)s   | 执行日志记录调用的源文件的路径名称 |
| %(funcName)s   | 执行日志记录调用的函数名称         |
| %(module)s     | 执行日志记录调用的模块名称         |
| %(lineno)s     | 执行日志记录调用的行号             |
| %(created)s    | 执行日志记录的时间                 |
| %(asctime)s    | 日期和时间                         |
| %(msecs)s      | 毫秒部分                           |
| %(thread)d     | 线程ID                             |
| %(threadName)s | 线程名称                           |
| %(process)d    | 进程ID                             |
| %(message)s    | 记录的消息                         |

```
- Python脚本中执行当前操作系统的命令的方法
  - os.chdir() 切换当前目录
	- os.system() 无返回结果 (打开一个子进程执行 命令)
	- os.popen()  可读取返回结果
```

### 2.2 日志模块的核心

- 四大核心
  - 日志记录器 Logger
  - 日志处理器Handler
  - 日志的过滤器Filter
  - 日志的格式化Formatter

## 三、scrapy框架

### 3.1 scrapy架构组成

- 五个核心

  - **engine 引擎**， 协调其它四个组件之间的联系，即与其它四个组件进行通信，也是scrapy框架的核心。

  - **spider 爬虫类**， 爬虫程序的编写代码所在， 也是发起请求的开始的位置。spider发起的请求，经过engine转入到scheduler中。

  - **scheduler 调度器**， 调度所有的请求（优先级高，则会先执行）。当执行某一个请求时，由engine转入到downloader中。
  - **donwloader 下载器**, 实现请求任务的执行，从网络上请求数据，将请求到的数据封装成响应对象，并将响应的对象返回给engine。engine将数据响应的数据对象（以回调接口方式）回传给它的爬虫类对象进行解析。
  - **itempipeline 数据管道**， 当spider解析完成后，将数据经engine转入到此（数据管道）。再根据数据类型，进行数据处理（图片、文本）

- 二个中间件

  - **爬虫中间件**， 介于Spider和Engine之间的，可以拦截Spider的发起的请求及数据。
  - **下载中间件**，介于Engine和Downloader之间的，可以拦截下载和响应。当然在下载处理之前，可以设置代理 、请求头、Cookie等操作（反反爬设置），还可以基于Splash或Selenium实现特定的操作。

### 3.2 scrapy指令

- 创建项目命令
  - scrapy startproject  项目名称
- 创建爬虫命令
  - scrapy genspider 爬虫名 域名
- 启动爬虫命令
  - scrapy crawl 爬虫名
- 调试爬虫命令
  - scrapy shell url
  - scrapy shell
    - fetch(url)

### 3.3 Response类

- 属性相关【重点】
  - body 响应的字节数据
  - text 响应的编码之后文本数据
  - headers 响应头信息， 是字节数据
  - encoding 响应数据的编码字符集
  - status 响应的状态码
  - url  请求的url
  - request 请求对象
  - meta 元数据，用于request和callback回调函数之间传值

- 解析相关【重点】

  - selector()

  - css()  样式选择器 , 返回Selector选择器的可迭代(列表)对象

    - scrapy.selector.SelectorList 选择器列表
      - x()/xpath()
    - scrapy.selector.Selector 选择器
    - 样式选择器提取属性或文本
      - `::text` 提取文本
      - `::attr("属性名")` 提取属性

  - xpath() xpath路径

    xpath路径，同lxml的xpath()写法

  - 选择器常用方法

    - css()/xpath()
    - extract()  提取选择中所有内容，返回是list
    - extract_first()/get() 提取每个选择器中的内容, 返回是文本

### 3.4 Request类

 - scrapy.http.Request

   请求对象的属性

   - url
   - callback 解释数据的回调函数对象
   - headers 请求头
   - priority 请求的优先级， 值越高，优先级越高（优先下载）

## 四、中午默写

- 写出selenium向下和向右滚动的脚本

  document.documentElement.scrollTop 向下

  document.documentElement.scrollLeft 向右

- 写出restful接口设计规范（四个）

  - 每个资源都有唯一标识 URI
  - 每个资源具有四个动作， GET|POST|PUT|DELETE
  - 每次请求都是无状态
  - 接口交互的数据是json或xml

- 写出常见的反爬虫和反反爬虫

  - 访问次数 - IP代理
  - Cookie验证- Cookie池
  - UA验证  - UA池
  - 验证码 - 打码平台
  - 动态js渲染 - Selenium/Splash

## 五、作业

- 爬取 http://www.ccgp-shaanxi.gov.cn/notice/list.do?noticetype=3&province=province 陕西省采购网
- 基于Flask实现日志上报服务器（日志微服务）
  - logging.handlers.HTTPHandler

