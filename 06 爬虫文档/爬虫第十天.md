# 爬虫第十天

## 一、回顾爬虫技术

### 1.1 网络请求

- urllib库

  - request.urlopen()|urlretrieve()|Request|build_opener()|HTTPHandler|HTTPCookieProcessor(http.cookiejar.Cookiejar())|ProxyHandler(proxies={})
  - parse.quote()|urlencode()

- requests库(接口测试)

  - 依赖urllib3(封装了很多类-OOB)
  - request(method, url, params, data, json, files, headers,cookies, auth, proxies)
  - get(url, params, headers, proxies)
  - post(url, data, json, headers, proxies, cookies,files)
  - put(url, data, json, headers, proxies, cookies, files)
  - delete(url)
  - session()  用于存储Cookie, 可以与服务器建立长连接`Connection: keep-alive`请求或响应头。

- 请求头和响应头

  - 请求头

    ```
    # 请求头原始报文的第一行
    # GET / HTTP/1.1
    HOST: www.baidu.com
    Accept: text/html,text/*
    Referer: 
    X-Requested-with: XMLHTTPRequest
    User-Agent: 
    Cookie: 
    Content-Type:
    Content-Length:
    ```

  - 响应头(Web后端服务)

    ```
    # 原始报文的第一行
    # HTTP/1.1 200 OK
    Content-Type: text/html;charset=utf-8
    Content-Length:
    Set-Cookie: 
    Date:
    Server: 
    Cookie: 
    ```

### 1.2 数据解析

- re解析提取

  - re.search()
  - re.findall()

- xpath提取(Element)

  - rootElement = lxml.etree.HTML(html_content)  节点对象
  - 节点对象的.xpath()提取数据
    - `/a/b/c`
    - `//div/ul//a`
    - `./li/a/@href | ./li/a/text()` 
    - `./li[1]`
    - `./li[position()<4]`
    - `./li[last()-1]`
    - `//div[@class="abc"]`
    - `//div[starts-with(@class, "abc")]`
    - `//div[ends-with(@class, "ddd")]`

- bs4( Tag )

  - rootTag = BeautifulSoup(html_content, 'lxml')

  - find('标签名'， class\_|id_)|find_all()
  - selector(CSS选择器)
  - bs4.element.Tag对象的属性
    - text|string|get_text()
    - attrs|Tag[]|Tag.get("属性名"， 默认属性值)
    - contents 所有的文本子节点
    - descendants 所有子标签节点对象

### 1.3 数据存储

- pymysql
- csv (csv.DictWriter|csv.DictReader)
- json
- excel( xlwt|xlrd )

### 1.4 爬虫框架

- Selenium (UI自动化测试工具)

  - 网络请求

  - 元素查找

    - find_element[s]_by_id|name|class_name|tag_name|xpath|css_selector()|link_text()

    - find_element(By, value) 查找第一个元素

      selenium.webdriver.common.by.By

    - find_elements(By, value) 查找所有元素

  - 事件交互（输入、点击、滚动、截屏、切换窗口）

  - 等待UI元素出现（用于等待Ajax加完数据）

    - selenium.webdriver.support.ui
    - selenium.webdriver.support.excepted_conditions as ec

    ```python
    ui.WebDriverWait(driver, timeout).until(
       ec.visibility_of_all_elements_located(
       	(By, value)
       ),
       timeout_msg
    )
    ```

- Scrapy|Scrapy-Redis

  - 五大核心组件两个中间件

    ```
    - Engin
    - Scheduler
    - Spider 
    - Downloader
    - ItemPipeline
    ```

    ```
    SpiderMiddlerware
    	- process_spider_input(self, response, spider)
    	- process_spider_output(self, response, results, spider)
    	- process_spider_exception(self, resposne, exct, spider)
    	- process_start_requests(self, start_reques
    	ts, spider)
    ```

    ```
    DownloaderMiddleware
    	- process_request(self, request, spider)
    	- process_response(self, request, response, spider)
    	- process_exception(self, request, excpt, spider)
    ```

    两个中间件都存在的方法

    ```
    @classmethod
    from_crawler(cls, crawler)
    ```

  - 解析数据时

    - response.css()|xpath()  返回SelectorList或Selector
    - Selector对象的方法
      - get()
      - extract() 
      - extract_first()
      - css()|xpath()

  - scrapy.Response对象的属性

    - status
    - encoding 可以指定字符集编码
    - headers
    - cookies
    - text 
    - body

  - scrapy.Request初始化参数

    - url
    - callback 指定回调函数
    - meta   向parse解析方法回传数据的dict类型的元数据
    - headers
    - cookies
    - priority  请求优先级
    - dont_filter 是否检查过滤重复的URL

  - 两个爬虫类

    - scrapy.Spider
      - 重写parse()
      - 指定start_urls = []
    - scrapy.spiders.CrawlSpider
      - 指定start_urls-> list 和 rules -> tuple
      - scrapy.spiders.Rule类初始化参数
        - link_extractor
          - scrapy.linkextractors.LinkExtractor 初始化参数
            - allow
            - deny
            - restrict_xpaths
            - restrict_css
        - callback: str
        - follow: bool

- 反爬虫的策略

  - UA （百度、安居）
  - Cookie
  - Referer（读书网的图片资源下载）
  - IP代理
  - 字体CSS加密(大众点评)
  - 图片验证码
  - 滑块验证码
  - 动态JS
  - 短信验证码

- 分布式爬虫

  - scrapy-redis 消息中间件使用redis

- 爬虫的部署【Linux熟悉】

  - 云服务器部署
  - docker部署
  - scrapyd部署

## 二、mongodb

### 2.1 docker部署

```
docker pull mongo
```

```
docker run -itd --name mongo_server1 -p 27017:27017 mongo
```

如果是在云服务启动的，则在`服务器的安全组规则`中添加`27017`端口访问的规则。

### 2.2 数据结构

- 文档：  指一条数据， 在javascript的以js对象来表示，在python以dict对象来表示。
- 集合:     指多条数据组成的对象，在javascript中以js数组表示，在python以list对象表示。
- 数据库： 多个集合组成了库



### 2.3 常用操作

- show dbs

- show collections

- use dushu  打开或创建dushu数据库

- db.createCollection('集合名')

- db.集合名.drop()

- db.dropDatabase()

- db.集合名.insert()|save()

- db.集合名.update(条件{}, 更新的数据{ $set: { }}, upsert, multi)

  - upsert 为真时，当条件没有匹配数据时，将更新的数据插入到集合中, 反之为假时，则什么也不做。
  - multi, 为真时， 当条件匹配多条数据时，将会更新所有的数据，反之，只更新每一条数据。

- db.集合名.remove(条件 { })

  删除user集合中文档的name属性包含`成`所有的记录

  ```js
  db.user.remove({name: {$regex: '成'}})
  ```

- db.集合名.find(条件{ }， 保留属性{name: 1}).pretty()

  - 逻辑关系
    - $or
    - $ne
    - $lt 
    - $gt
    - $lte
    - $gte
  - 正则表示
    - $regex

## 三、Anaconda

## 四、Jupyter基本用法

## 五、中午默写

- 写出urllib、requests和scrapy的response响应对象的类及属性

  - urllib:   http.client.HTTPResponse
    - code
    - getheader(name, default_value)
    - getheaders()
    - read()|readline()|readlines()
  - requests:  requests.Response
    - status_code
    - headers
    - encoding
    - cookies
    - text|contents
  - scrapy:     scrapy.http.Response
    - status
    - headers
    - cookies
    - encoding
    - text|body
    - request
    - meta
    - url

- 写出lxml的xpath和scrapy的xpath的不同之处

  - lxml的xpath
    - 返回元素对象是Element, list[Element, Element, ]
    - 提取元素属性或文本时，返回list['', '']

  - scrapy的xpath
    - 返回元素对象是SelectorList或Selector
    - 提到元素的属性或文本时，返回SelectorList或Selector
    - 元素对象提取数据的方法:  .get()|extract()|extract_first()

- 写出Dockerfile的RUN和CMD的用法与区别

  - RUN 在容器中执行普通的命令， 在Dockerfile中可以多次使用
  - CMD 在容器启动时执行的命令， 在Dockerfile只能执行一次（最后一个命令）

## 六、作业

- 基于flask实现笑话网的查询、推荐接口(5个笑话)

  搜索页面： 类似于百度首页

  推荐接口:  根据搜索记录，推荐5个笑话

- Docker实现MongoDB的集群

- 整理2周爬虫的知识点

- 复习Shell脚本编程

- 安装Anaconda环境，自我学习Conda命令

- 提前学习jupyter notebook的常见的快捷方式