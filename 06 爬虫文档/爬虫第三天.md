# 爬虫第三天

## 一、回顾知识点

### 1.1 requests库

- requests.request(method, url, **kwargs)

  常用的参数

  - params/data/json   上传数据
  - files  上传文件
  - headers/cookies
  - proxies  代理服务器
  - auth 授权

- requests.get(url, params, **kwargs)

- requtests.post(url, data, json, **kwargs)

- requests.put(url, data, json, **kwargs)

- requests.delete(url, **kwargs)

- requests.session()  - > session对象， 可以调用 s.get()/post()/put()/delete()等方法，多次请求的会话(连接Session)是同一个

 所有的请求返回的对象是requests.Response类的实例, 实例的属性：

- status_code
- headers
- encoding
- text/content
- cookies
- json()  反序列化json文本字符串为python的list或dict的对象

### 1.2 xpath解析

- 路径写法
  - `/` 依次查找
  - `//` 间接查找
  - `./` 从当前元素下查找
  - `.//` 从当前元素的间接子节点查找
- 位置条件
  - `//li[1]`  整个文档中的第一个`<li>`标签
  - `//li[last()]` 最后一个
  - `//li[position() < 3]`  前2个
  - `//li[position() - 2]` 倒数第2个
- 属性条件
  - `//li[@id="xxxx"]`
  - `//li[@class=""]`   @class 属性名
  - `//li[@class="" and @name=""]`  多个属性的且的关系
- 同时提取两个元素
  - `//title/text()  | //img/@src`
- 模糊条件
  - `//div[contains(@class, "page")] ` 查找class属性包含page的所有div标签
  - `//div[starts-with(@class, "box")]`  第一个class的属性值为box的div标签
  - `//div[ends-with(@class, "clearfix")]`最一个class的属性值为clearfix的div标签

### 1.3 扩展封装ES-SDK

```python
"""
基于requests库封装操作ElasticSearch搜索引擎的函数 (SDK)
"""
from urllib.parse import quote

import requests

INDEX_HOST = '119.3.170.97'
INDEX_PORT = 80


class ESIndex():
    """ES的索引库的类"""

    def __init__(self, index_name, doc_type):
        self.index_name = index_name
        self.doc_type = doc_type

    def create(self):  # 创建索引库
        url = f'http://{INDEX_HOST}:{INDEX_PORT}/{self.index_name}'
        json_data = {
            "settings": {
                "number_of_shards": 5,
                "number_of_replicas": 1
            }
        }
        resp = requests.put(url, json=json_data)
        if resp.status_code == 200:
            print('创建索引成功')
            print(resp.json())

    def delete(self):  # 删除索引库
        resp = requests.delete(f'http://{INDEX_HOST}:{INDEX_PORT}/{self.index_name}')
        if resp.status_code == 200:
            print('delete index ok')

    def add_doc(self, item: dict):
        # 向库中增加文档
        doc_id = item.pop('id', None)
        url = f'http://{INDEX_HOST}:{INDEX_PORT}/{self.index_name}/{self.doc_type}/'
        if doc_id:
            url += str(doc_id)

        resp = requests.post(url, json=item)
        if resp.status_code == 200:
            print(f'{url} 文档增加成功!')

    def remove_doc(self, doc_id):
        # 删除文档
        url = f'http://{INDEX_HOST}:{INDEX_PORT}/{self.index_name}/{self.doc_type}/{doc_id}'
        resp = requests.delete(url)
        if resp.status_code == 200:
            print(f'delete {url} ok')

    def update_doc(self, item: dict):
        # 更新文档
        doc_id = item.pop('id')
        url = f'http://{INDEX_HOST}:{INDEX_PORT}/{self.index_name}/{self.doc_type}/{doc_id}'
        resp = requests.put(url, json=item)
        assert resp.status_code == 200
        print(f'{url} update ok')

    def query(self, wd=None):
        # 查询
        q = quote(wd) if wd else ''
        url = f'http://{INDEX_HOST}:{INDEX_PORT}/{self.index_name}/_search?size=100'
        if q:
            url += f'&q={q}'
        resp = requests.get(url)
        datas = []
        if resp.status_code == 200:
            ret = resp.json()
            hits = ret['hits']['hits']
            if hits:
                for item in hits:
                    data = item['_source']
                    data['id'] = item['_id']

                    datas.append(data)

        return datas


if __name__ == '__main__':
    index = ESIndex('gushiwen', 'tuijian')
    # index.create()
    # index.add_doc({
    #     'id': 1,
    #     'name': 'disen',
    #     'price': 19.5
    # })
    #
    # index.add_doc({
    #     'id': 2,
    #     'name': 'jack',
    #     'price': 10.5
    # })

    print(index.query())
```

## 二、正则解析数据

### 2.1 扩展Linux文件权限

```
100  ->4 -> r

010 -> 2-> w

001 -> 1 -> x



100 | 010 = 110  # 增加权限

110 & 100 == 100  # 验证100权限

110 ^ 100 = 010   # 删除100权限 
```

### 2.2 re面试中的问题

- compile() /match()/search() 三者之间的区别
- search()/findall()区别
- 贪婪模式和非贪婪模式



### 2.3 解析站长之家

```python
"""
基于正则re模块解析数据
"""
import re
import os

import requests

from utils.header import get_ua

base_url = 'http://sc.chinaz.com/tupian/'
url = f'{base_url}shuaigetupian.html'

headers = {
    'User-Agent': get_ua()
}

if os.path.exists('mn.html'):
    with open('mn.html', encoding='utf-8') as f:
        html = f.read()
else:
    resp = requests.get(url, headers=headers)
    print(resp.encoding)  # IOS-8859-1
    resp.encoding = 'utf-8'  # 可以修改响应的状态码
    assert resp.status_code == 200
    html = resp.text
    with open('mn.html', 'w', encoding=resp.encoding) as f:
        f.write(html)

# print(html)
#  [\u4e00-\u9fa5]
compile = re.compile(r'<img src2="(.*?)" alt="(.*?)">')
compile2 = re.compile(r'<img alt="(.*?)" src="(.*?)">')

imgs = compile.findall(html)  # 返回list
if len(imgs) == 0:
    imgs = compile2.findall(html)

print(len(imgs), imgs, sep='\n')

# 下一页
next_url = re.findall(r'<b>20</b></a><a href="(.*?)" class="nextpage"',html, re.S)

print(base_url+next_url[0])
```

## 三、多任务爬虫



## 四、BS4解析数据



## 五、中午默写

- 写出requests.request()方法常用的参数及参数类型

  - method: str 请求方法， 可以指定 get, post, put, delete, options

  - url : str 请求路径或api接口
  - params/data/json  :  dict 上传的请求参数及json或form的data数据
  - headers/cookie: dict 请求头或Cookie信息
  - files: dict 上传的文件信息

- 写出正则的贪婪模式有哪些

  - `.*`    0或多个任意字符
  - `.+`  1或多个任意字符
  - `.?` 
  - `.{n, }` 至少n个以上的任意字符
  - `.{n, m}`  至少n个以上的任意字符

- 写出str对象的常用方法（10+）

  - join()

  - split() 

  - strip()

  - replace()

  - upper()

  - lower()

  - title()

  - index()/rindex()

  - find()/rfind()

  - insert()

  - just()/ljust()/rjust()

  - capitalize()  # 每个单词的首字母大写

    ```
    要求： 自动生成订单号
    订单号的格式： 20191226000001
    当天的单号按自增，第二天的序号是从1开始。
    ```

  - count()

  - encode()

  - startswith()/endswith()

  - format()



## 六、作业

1. 基于Flask实现文件上传服务器， 通过requests测试文件上传接口。
2. 优化美女网爬虫，将数据存到es搜索引擎中
3. 完成站长之家的多任务爬虫的数据存储（ES引擎/csv）