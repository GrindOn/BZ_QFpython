# 爬虫第二天

## 一、回顾知识点

核心的网络请求库 -> urllib库

- urllib.request 模块

  - urlopen(url | request: Request,  data=None)  data是bytes类型
  - urlretrieve(url,  filename) 下载url的资源到指定的文件
  - build_opener(*handlder)  构造浏览器对象
    - opener.open(url|request, data=None)  发起请求
  - Request 构造请求的类

  ```python
  data={
    'wd': '千锋'
  }
  # urlencode(data)  => 'wd=%f5%e6%e6%f5%e6%e6'
  request = Request(url, data=urlencode(data).encode())
  ```

  - HTTPHandler HTTP协议请求处理器
  - ProxyHandler(proxies={'http': 'http://proxy_ip:port'}) 代理处理
  - HTTPCookieProcessor(CookieJar())
    - http.cookiejar.CookieJar 类

- urllib.parse模块

  - quote(txt)  将中文字符串转成url编码
  - urlencode(query: dict) 将参数的字典转成url编码，结果是key=value&key=value形式，即以 `application/x-www-form-urlencoded`作为url编码类型。

## 二、requests库【重点】

> requests库也是一个网络请求库， 基于urllib和urllib3封装的便捷使用的网络请求库。

### 2.1 安装环境

```
pip install requests -i https://mirrors.aliyun.com/pypi/simple
```

### 2.2 核心的函数

- requests.request()  所有请求方法的基本方法

  以下是request()方法的参数说明

  - method: str 指定请求方法， GET, POST, PUT, DELETE

  - url: str  请求的资源接口（API），在RESTful规范中即是URI(统一资源标签识符)

  - params:  dict ， 用于GET请求的查询参数(Query String params)；

  - data: dict , 用于POST/PUT/DELETE 请求的表单参数(Form Data)

  - json: dict 用于上传json数据的参数， 封装到body（请求体）中。请求头的Content-Type默认设置为`application/json`

  - files: dict, 结构 {'name': file-like-object | tuple}, 如果是tuple， 则有三种情况：

    - ('filename', file-like-object)
    - ('filename', file-like-object, content_type)
    - ('filename',  file-like-object, content_type, custom-headers)

    指定files用于上传文件， 一般使用post请求，默认请求头的`Content-Type`为`multipart/form-data`类型。

  - headers/cookies ： dict

  - proxies: dict , 设置代理

  - auth: tuple , 用于授权的用户名和口令， 形式('username', 'pwd')

- requests.get()  发起GET请求， 查询数据

  可用参数：

  - url
  - params
  - json
  - headers/cookies/auth

- requests.post() 发起POST请求， 上传/添加数据

  可用参数：

  - url
  - data/files 
  - json
  - headers/cookies/auth

- requests.put()  发起PUT请求， 修改或更新数据

- requests.patch()  HTTP幂等性的问题，可能会出现重复处理， 不建议使用。用于更新数据

- requests.delete() 发起DELETE请求，删除数据

### 2.3 requests.Respose

以上的请求方法返回的对象类型是Response， 对象常用的属性如下：

- status_code 响应状态码
- url  请求的url
- headers : dict 响应的头， 相对于urllib的响应对象的getheaders()，但不包含cookie。
- cookies： 可迭代的对象，元素是Cookie类对象（name, value, path）
- text : 响应的文本信息
- content: 响应的字节数据
- encoding: 响应数据的编码字符集， 如utf-8, gbk, gb2312
- json():  如果响应数据类型为`application/json`，则将响应的数据进行反序化成python的list或dict对象。
  - 扩展-javascript的序列化和反序列化
    - JSON.stringify(obj) 序列化
    - JSON.parse(text) 反序列化



## 三、数据解析方式之xpath

> xpath属于xml/html解析数据的一种方式， 基于元素（Element）的树形结构(Node > Element)。选择某一元素时，根据元素的路径选择，如 `/html/head/title`获取`<title>`标签。

### 3.1 绝对路径

从根标签开始，按tree结构依次向下查询。

如 /html/body/table/tbody/tr。

### 3.2 相对路径

相对路径可以有以下写法

- 相对于整个文档

  ```
  //img
  ```

  查找出文档中所有的`<img>`标签元素

- 相对于当前节点

  ```
  //table
  ```

  假如当前节点是`<table>`, 查找它的`<img>`的路径的写法

  ```
  .//img
  ```

### 3.3 数据提取

- 提取文本

  ```
  //title/text()
  ```

- 提取属性

  ```
  //img/@href
  ```

### 3.4 位置条件

获取网页中的数据类型与字符集, 获取第一个`<meta>`标签

```
//meta[1]//@content
```

获取最后一个`<meta>`标签

```
//meta[last()]//@content
```

获取倒数第二个`<meta>`标签

```
//meta[position()-2]//@content
```

获取前三个`<meta>`标签

```
//meta[position()<3]//@content
```

### 3.5 属性条件

查找 class为`circle-img`的`<img>`标签

```
//img[@class="circle-img"]
```



### 3.6 在Python中应用

安装包 pip install lxml



## 四、中午默写

- 写出urllib库的请求处理器有哪些类（尽量写全路径）
  - urllib.request.HTTPHandler
  - urllib.request.HTTPCookieProcessor
  - urllib.request.ProxyHandler
- 写出json.loads()和pickle.loads()返回的数据类型
  - json.loads() 返回list或dict， 加载的是字符串
  - pickle.loads() 返回是python中的对象， 加载的是字节数组 bytes
- 写出pymysql的cursor.execute()方法中的参数及作用
  - 有两个参数， 一个是sql, 一个是args
  - args可以是tuple，对应sql字符串的 `%s`
  - args也可以是dict, 对应sql字符串的`%(xxx)s`,  xxx是dict中的key



## 五、作业

1. 买家秀的模特的所有图片， 图片的名称是姓名-序号， 如'Disen-1.jpg', 'Disen-2.jpg'

2. 中国图书网

   爬虫所有的小说的基本信息（名称、作者、出版社、原价、折扣价、活动标签、 简介）



  **扩展任务**

- 在Ubuntu 下安装docker
- 基于docker部署ElasticSearch搜索引擎库。
- 基于requests实现索引库及文档的添加和查询。

