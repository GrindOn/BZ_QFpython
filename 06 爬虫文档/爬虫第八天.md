# 爬虫第八天

## 一、回顾知识点

### 1.1 数据处理

- 启动爬虫的指令中带有 -o参数， 指定数据存储的文件(csv/json/xml)位置

- 数据管道 pipeline

  - settings.py中配置管道类

  - process_item(self, item, spider) 管道类的处理数据的方法

    - 通过**isinstance()** 判断item属于哪一种类型, 然后按某一类型进行处理

    - 扩展 Python的自省相关函数

      ```python
      type(obj)  获取对象的类型
      dir(obj)   获取对象中所有的属性(函数、类、变量)
      help(obj)  获取对象的文档帮助信息
      isinstance(obj, 类) 判断obj是否为类的实例对象
      issubclass(类， 父类) 判断类是否为父类的子类
      hasattr(对象， 属性或方法)  判断对象中是否存在属性或方法
      getattr(对象， 属性或方法)  获取对象中的属性或方法
      setattr(对象， 属性， 属性值) 设置对象的属性或方法
      id(对象) 获取对象在内存中的唯一标识
      ```

    - 返回item：  目的是让优先级低的数据管道类可以接收到item数据。

### 1.2 中间件

- 爬虫中间件（scrapy工作流中第1， 第6， 第7等3个步）

  ```python
  @classmethod
  def from_crawler(self, crawler)
  
  def process_spider_input(self, response, spider)
  
  def process_spider_output(self, response, result, spider)
  
  def process_spider_exception(self, response, exception, spider)
  
  def process_start_requetst(self, start_requests, spider)
  
  def spider_opened(self, spider)
  ```

  

- 下载中间件 (第4， 5两个步骤)

  ```python
  @classmethod
  def from_crawler(self, crawler)
  
  def process_request(self, request, spider):
    return None|Request|Response| raise IgnoreRequest
  
  def process_response(self, request, response, spider)
  	return response|request
    
  def process_exception(self, request, excecption, spider)
  
  def spider_opened(self, spider)
  ```



### 1.3 规则爬虫

- 创建规则爬虫的指令

  ```
  scrapy genspider -t crawl 爬虫名 域名
  ```

- 链接提取器 LinkExctractor

  - 正则方式 （allow | deny）
  - restrict_xpaths() xpath方式指定`a`标签所在的（间接）父级标签
  - restrict_css() 样式方式指定`a`标签所在的（间接）父级标签

- Rule() 规则

  - extractor: LinkExtractor
  - callback： str
  - follow=True 表示提取的连接在请求成功后，解析时是否继续按此规则提取连接

- 不能重写parse()函数 【注】

## 二、规则爬虫【重】

### 2.1 LinkExtractor 类

作用： 提取感兴趣的`a`标签中的连接`href`属性， 因此在指定正则表过式，参考某些`a`标签中`href`属性的写法。如果正则提取困难，则支持css或xpath两个方式来指定`a`标签所在的父级标签。

### 2.2 核心的类

-  scrapy.spiders.CrawlSpider 规则爬虫类

  重写了parse()解析函数，在此函数中通过指定规则中的LinkExtractor对象来提取当前响应数据中的连接，并向engine发起新的请求。新的请求中包含提取的连接url和rule中的回调函数。

  ```python
  def parse(self, response):
          return self._parse_response(response, 
          		self.parse_start_url, cb_kwargs={}, follow=True)
  
   
  def _parse_response(self, response, callback, cb_kwargs, follow=True):
      if callback:
          cb_res = callback(response, **cb_kwargs) or ()
          cb_res = self.process_results(response, cb_res)
          for requests_or_item in iterate_spider_output(cb_res):
              yield requests_or_item
  
      if follow and self._follow_links:
          for request_or_item in self._requests_to_follow(response):
              yield request_or_item
              
  def _requests_to_follow(self, response):
      if not isinstance(response, HtmlResponse):
          return
      seen = set()
      for n, rule in enumerate(self._rules):
          links = [lnk for lnk in rule.link_extractor.extract_links(response)
                   if lnk not in seen]
          if links and rule.process_links:
              links = rule.process_links(links)
          for link in links:
              seen.add(link)
              r = self._build_request(n, link)
              yield rule.process_request(r)    
  ```

- scrapy.spiders.Rule 规则类

  - extractor: LinkExtractor
  - callback:str
  - follow:bool

- scrapy.linkextractors.LinkExtractor 链接提取器类

  - allow
  - deny
  - restrict_xpaths
  - restrict_css

- 创建规则爬虫是使用 -t crawl 模板

  ```
  scrapy genspider -t crawl 爬虫名 域名
  ```

## 三、图片管道

### 3.1 使用ImagesPipeline

- settings.py配置

  - IMAGES_STORE 指定数据存放的位置

  - 在`ITEM_PIPELINES`字典中，引入`scrapy.pipelines.images.ImagesPipeline`

  - 配置缩略图

    ```python
    IMAGES_THUMBS = {
    	 'small': (with, height),
    	 'big': (widht, height)
    }
    ```

    

- item数据属性

  - image_urls: list  表示下载图片地址的列表
  - images: list 表示下载完成后的图片存放在`IMAGES_STORE`中的路径及相关的属性



### 3.2 自定义ImagesPipeline

> 实现ImagesPipeline的子类，重写三个核心的方法。

三个核心的方法：

- get_media_requests(self, item, info)  根据item中图片连接的属性，返回图片下载的request。

  可以返回一个Request也可以多个Request的列表

- file_path(self, request, response, info)  根据请求和响应返回图片保存的位置（相对于IMAGES_STORE）。

  如果返回的路径中包含子路径时，系统会自动创建子目录( os.makedirs() )。

- item_completed(self, results, item, info) 图片下载完成后，从results的结果获取图片的保存路径，并设置到item中，最后返回这个item。

  results格式 

  ```python
  [
     (True,  {'path': '', 'url': '', chucksum: ' '} ),
     (True,  {'path': '', 'url': '', })
  ]
  ```

  

## 四、其它技术点

### 4.1 日志

> scrapy的日志记录器在爬虫类对象中， 是logger, 通过爬虫对象的logger来记录运行的信息。scrapy的logger日志记录器使用了Adpater设计模式的logging.LoggerAdapter类。

【扩展】构建器设计模式 Builder ( 函数式-流式编程 )

```
car: Car = CarBuilder().step1().step2().step3().step4().build()
```

```
text = response.xpath().css().xpath().css().xpath().get()
```

Python开发人员需要掌握的设计模式： 单例、工厂、装饰器、适配器、构建器、生产者消费者（消息队列-订阅/发布）。

#### 4.1.1 配置

在settings.py文件中，指定收集日志的等级及日志存储的文件名

```
LOG_LEVEL = ''  # DEBUG|INFO|ERROR|WARNING|CRITICAL
LOG_FILE = '文件名'  # os.path.join(BASE_DIR, 'access.log')
```

#### 4.1.2 在程序中使用

- 爬虫日志记录器

```
spider.logger.info()/error()
```

- 获取scrapy中其它的记录器

  - 内置的记录器

  ```
  - scrapy.utils.log
  - scrapy.crawler
  - scrapy.middleware
  - scrapy.downloadermiddlewares.httpauth|downloadtimeout...
  - scrapy.core.engine
  - scrapy.utils.signal
  ```

  ```
  logging.getLogger('scrapy.utils.log').info()/warning()
  ```

- 自定义记录器

  ```python
  error_logger = logging.getLogger('dushu-project')
  error_logger.setLevel(logging.ERROR)
  
  handler = logging.FileHandler(
      os.path.join(BASE_DIR, 'error.log'),
      encoding='utf-8'
  )
  handler.setLevel(logging.ERROR)
  handler.setFormatter(logging.Formatter(
      '%(asctime)s %(name)s at %(lineno)s of %(pathname)s : %(message)s'
  ))
  error_logger.addHandler(handler)
  ```

  使用时

  ```python
  from dushu.settings import error_logger
  
  error_logger.error('消息')
  ```

### 4.2 post请求

- scrapy.http.FormRequest

  - url
  - formdata: dict( value必须都是字符串类型)
  - cookies: dict
  - headers: dict
  - callback 默认parse

- 如果爬虫中第一次的请求是post请求,则重写Spider类的start_requests()方法

  ```python
   def start_requests(self):
          self.url = 'http://ccgp-shaanxi.gov.cn/notice/noticeaframe.do?noticetype=3&province=province&isgovertment='
          self.data = {
              'page.pageNum': '1'
          }
          self.MAX_PAGE = 1399
          yield FormRequest(self.url, formdata=self.data)
  ```

  不需要指定start_urls 列表。

### 4.3 Selenium中间件

作用： 动态渲染js(ajax加载数据)

#### 4.3.1 定义下载中间件类

```python
class LoadDataMiddleware():
    @classmethod
    def from_crawler(cls, crawler):
        # This method is used by Scrapy to create your spiders.
        s = cls()
        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)
        crawler.signals.connect(s.spider_closed, signal=signals.spider_closed)

        return s
```

- 连接打开爬虫类的信号，在处理函数中打开chrome

```python
    def spider_opened(self, spider):
        # 创建Selenium的Chrome浏览器对象
        # chromedriver.exec 所在的目录已配置到环境变量Path中
        options = Options()
        options.add_argument('--headless')
        options.add_argument('--disable-gpu')

        self.chrome = Chrome(options=options)
```

- 连接关闭爬虫类的信号，在处理函数中退出chrome

- ```python
   def spider_closed(self, spider):
          self.chrome.quit()
  ```

- 处理process_request()方法编写核心业务

  ```python
   def process_request(self, request, spider):
          # 判断是否为第一次请求
          if not request.meta.get('next_page', False):
              self.chrome.get(request.url)
          else:
              # 点击下一页
              self.chrome.execute_script('var q=document.documentElement.scrollTop=1200')
              time.sleep(0.5)
              self.chrome.execute_script('var q=document.documentElement.scrollLeft=1000')
              time.sleep(0.5)
              self.chrome.find_elements_by_xpath('//ul[@class="pagination"]/li/a')[-2].click()
  
          time.sleep(2)
          html = self.chrome.page_source
          return HtmlResponse(request.url, body=html.encode('utf-8'))
  ```

#### 4.3.2 配置

```python
DOWNLOADER_MIDDLEWARES = {
   'caigou.middlewares.LoadDataMiddleware': 543,
}
```

#### 4.3.3 爬虫类

```python
class ShaanxiSpider(scrapy.Spider):
    name = 'shaanxi2'
    allowed_domains = ['ccgp-shaanxi.gov.cn']
    # start_urls = ['http://ccgp-shaanxi.gov.cn/notice/noticeaframe.do?noticetype=3&province=province&isgovertment=']
    start_urls = ['http://ccgp-shaanxi.gov.cn/notice/list.do?noticetype=3&province=province']

    def parse(self, response):
        trs = response.css('.list-box tbody tr')
        for tr in trs:
            item = {}
            item['id'] = tr.xpath('./td[1]/text()').get()
            item['area'] = tr.xpath('./td[2]/text()').get()
            item['title'] = tr.xpath('./td[3]/a/text()').get()
            item['url'] = tr.xpath('./td[3]/a/@href').get()
            item['date'] = tr.xpath('./td[4]/text()').get()

            yield item

        # 获取下一页数据
        if len(trs) == 15:
            yield Request(response.request.url,
                          meta={'next_page': True}, dont_filter=True)
```

dont_filter=True 原因是可能会被认为是重复， 让engine不去过滤重复的url。

## 五、中午默写

- ### 定量爬虫的指令有哪些

  ```
  CLOSESPIDER_ITEMCOUNT  item数据条目量
  CLOSESPIDER_PAGECOUNT  请求成功响应的次数
  ```

- ### 下载中间件类的处理请求的方法是什么，可以返回哪些对象

  ```python
  def process_request(self, request, spider):
      return None|Request|Response | rasie IgnoreRequest
  ```

  - None 表示不拦截当前的请求
  - Request 重新返回新的请求， engine将这个新请求压入到Scheduler中
  - Response 表示不需要下载器下载，由自己程序下载并且封装成Response对象。
  - raise IgnoreRequest 取消当前的request请求(重复， 过滤的)

- ### 简述中间件from_crawler(self, crawler)函数的作用

  ```
  当爬虫程序启动后，用于创建当前中间件类实例的方法。
  可以监听爬虫程序是否正常启动。
  ```

  【扩展】类方法、静态方法和实例方法的区别？？

  ```
  类方法：类对象（由元类创建类的对象）的方法，第一个参数是cls(表示当前类本身)。
  	     @classmethod 修饰的方法
  类实例方法： 由类的__new__()函数处理的类实例对象的方法，第一个参数是self(表示当前类的实例)
  静态方法： 和类没有任何关系，只是在当前类中声明，方法的参数不会出现cls和self。
           @staticmethod 修改的方法
  ```

  【扩展】什么是抽象方法？

  ```
  抽象方法是在父类中声明（没有实现功能），由子类实现的方法。如果子类中未实现，则会在调用时报错。
  ```

  ```python
  class Animal():
    def eat(self):  # 抽象方法
       raise Exception('子类中必须实现此方法！')
  ```

  ```python
  class Pig(Animal):
     def eat(self):
        print('Pig eat ', 'ddd')
        
  class Dog(Animal):
     def eat(self):
        print('Dog eat ', 'abc')
  ```

  

## 六、作业

- 开心笑笑网的爬虫【儿童笑话】

  https://www.gbfzh.com/ertong/

- 电影天堂

  https://www.dytt8.net/html/gndy/dyzz/index.html 