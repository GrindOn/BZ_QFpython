# 爬虫第九天

## 一、回顾知识点

### 1.1 规则爬虫

- scrapy.spiders.CrawlSpider  所有规则爬虫类的父类

- scrapy.spiders.Rule 规则类

  - LinkExtractor 连接提取类的实例对象
  - callback： str 指定提取器提取的连接请求成功之后的数据解析的函数
  - follow: bool 表示对提取连接请求成功后的数据是否继续提取连接

- scrapy.linkextractors.LinkExtractor

  - allow: str  提取连接中的href的正则表示
  - deny: str 拒绝提取连接中的href的正则表示
  - restrict_xpaths  以xpath的路径方式指定`a`所在的父级标签
  - restrict_css   以css的样式选择器方式指定`a`所在的父级标签

- 创建规则爬虫的指令

  ```
  scrapy genspider -t crawl 爬虫名 域名
  ```

### 1.2 日志

- logging.LoggerAdapter  日志适配器类， 封装日志记录器和spider等相关消息

- 每个爬虫类都存在它的日志记录器，记录器名称即是`爬虫名`（self.name）

- 在中间件或管道等相关处理数据（item/request/response）的方法中都存在spider对象

  ```
  spider.logger.info()/error()..记录日志
  ```

- 在settings.py中配置日志记录的等级及日志文件存储的位置

  ```python
  LOG_LEVEL='INFO'
  LOG_FILE = '/var/log/xxx.log'
  ```

- 可以自定义日志记录器（可以声明一个日志处理模块-`log_`）

  ```
  dushu
    |---dushu
    		  |---spiders
    		  |---item.py
    		  |---middleware.py
    		  |---pipeline.py
    		  |---settings.py
    		  |---log_
    		  		|--__init__.py
    |---scrapy.cfg
  ```

  如`log_.__init__.py`文件中的内容：

  ```python
  import logging
  from logging import Formatter, FileHandler
  
  logger = logging.getLogger('dushu')
  logger.setLevel(logging.INFO)
  
  handler = FileHandler('/var/log/dushu.log')
  handler.setLevel(logging.INFO)
  handler.setFormatter(Formatter(format='%(asctime)s:  %(message)s', 
                                 datefmt='%Y-%m-%d %H:%M:%S'))
  
  logger.addHandler(handler)
  ```

  ```python
  from dushu.log_ import logger
  
  logger.info('记录的信息')
  ```

### 1.3 ImagesPipeline

- scrapy.pipelines.images.ImagesPipeline  配置在settings.py的`ITEM_PIPELINES={}`中

- 在settings.py文件配置图片存储的位置

  ```
  IMAGES_STORE = '/var/src/images'
  ```

- 在爬虫类的parse()中，指定item的`image_urls`和`images`

- 自定义ImagesPipeline

  - 声明ImagesPipeline的子类

  - 重写get_media_requests(self, item, info)方法, 在此方法中获取item的图片下载地址，返回相关的Request(url, meta={'name': item['name']})

  - 重写file_path(self, request, response, info)方法， 在此方法中返回相对于IMAGEs_STORE图片存储的相对路径

    ```python
    return '%s/%s.jpg' %(dir_name, file_name)
    ```

  - 重写item_completed(self, results, item, info)，在此方法中，从results中获取下载文件存储的路径， 并添加到item中

    ```python
    path = [ data['path'] for ok, data in results if ok]
    item['path'] = ','.join(path)
    
    return item
    ```

### 1.4 Selenium下载中间件

- 声明一个类， 并将此类配置在`DOWNLOADER_MIDDLEWARES={}`

- 声明`from_crawler(self, crawler)` 创建当前中间件类的实例的， 在此方法中指定`spider_opened`和`spider_closed`两个信号的处理方法

  - 在spider_opened()方法中，创建Chrome浏览器的驱动对象
  - 在spider_closed()方法， 将chrome浏览器对象进行退出

- 声明`process_request(self, request, spider)` 处理每个请求

  ```python
  self.chrome.get(request.url)
  # ....等待某个UI元素出现
  
  html = self.chrome.page_sources
  
  return scrapy.http.HtmlResponse(request.url, body=html.encode('utf-8'))
  ```

  

## 二、分布式爬虫

### 2.1 什么是分布式

- Hadoop 分布式计算框架（大数据处理）HDFS（分布式文件系统）
  - MapReduce
  - Hbase 数据库
  - Hive  实时数据库
  - Spark 大数据平台（MySQL、Hbase）
- 由多个服务器（操作系统-PC）组成，在调度器调度的情况下完成不同的任务，这种架构称之为分布式。常见的调度器是消息中间件、服务注册中心、负载均衡等组成。

### 2.2 常见消息队列

- Redis订阅与发布-实现消息队列
- RabbitMQ 基于Channel实现消息队列
- Kafka 消息队列

### 2.3 scrapy-redis

- 安装包:  pip install scrapy-redis

- 配置调度器类、去重类及消息中间件（队列的位置）

  ```
  SCHEDULER="scrapy_redis.scheduler.Scheduler"
  SCHEDULER_PERSIST=True
  DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"
  
  REDIS_URL='reids://[:password@]host-ip:6379/1' # 0-15数据库索引
  ```

-  修改爬虫类

  - 父类 scrapy_redis.spiders.RedisSpider|RedisCrawlSpider
  - 去掉start_urls 列表
  - 增加redis_key 字符串变量， 指定redis服务中存储的key

- 按正常启动爬虫程序命令启动爬虫

  ```
  scrapy crawl 爬虫名
  ```

- 连接redis服务，向redis_key的列表list中推送请求任务

  ```
  lpush xxxx http://www.xxxx.com/xxx/
  ```

  

## 三、爬虫程序部署

### 3.1 scrapyd

- 安装scrapyd服务和客户端

  ```
  pip install scrapyd scrapyd-client
  ```

- 如果在云服务器安装scrapyd

  - 修改scrapyd源码中app.py文件

    ```
    bind_addres = '0.0.0.0'
    ```

  - 在云服务的安全组中，放开6800端口

- 在scrapyd服务启动Python环境中安装爬虫需要的依赖库

- 修改爬虫项目scrapy.cfg文件

  ```
  [deploy:100]
  url = http://119.3.170.97:6800/
  project = dushu_redis
  ```

- 使用scrapyd-deploy命令发布项目

  ```
  scrapyd-deploy 100 -p dushu_redis
  ```

- 通过scrapyd的接口启动和停止爬虫

  - http://119.3.170.97:6800/scheduler.json  启动爬虫接口

    - post请求
    - project 项目名参数
    - spider 爬虫名参数

    请求成功后，返回json数据，包含job_id数据，用于停止爬虫。

  - http://119.3.170.97:6800/cancel.json 停止爬虫接口

    - post请求
    - job 队列的ID

### 3.2 docker部署

#### 3.2.1 编写Dockerfile文件

```dockerfile
FROM 119.3.170.97:5000/ubuntu:latest
MAINTAINER disen 610039018@qq.com
ADD . /usr/src
WORKDIR /usr/src
VOLUME /usr/src
RUN pip install -r requirements.txt -i https://mirrors.aliyun.com/pypi/simple
RUN chmod +x run.sh
CMD /usr/src/run.sh
```

- ADD 命令将当前目录下所有文件复制到容器的`/usr/src`目录下
- WORKDIR 容器中切换当前的工作目录，类似于`cd`
- VOLUME 将容器中的文件位置暴露给外部宿主机， 在启动镜像时，通过 `docker run -v`同步宿主和容器之间的文件目录
- RUN 执行容器中的命令， 可以执行多次
- CMD 是当容器启动时执行的命令，且Docker中只能使用一次

#### 3.2.2 编写shell文件

```sh
#!/bin/bash
cd /usr/src
scrapy crawl guoxue
```

#### 3.2.3 构建镜像

在执行`docker build`之前，需要将sh文件改为可执行， 且文件格式改为UNIX。

vi run.sh文件，按`shift+:`进入命令行，执行以下指令：

```
:set ff=unix
```

```sh
docker build -t dushu:1.0 .
```

成功之后，查看镜像是否存在

```
docker images
```

#### 3.2.4 启动镜像

```
docker run -itd --name spider_dushu -v /root/dushu_spider:/usr/src dushu:1.0
```

查看日志是否启动

```
cat dushu.log
```

```
docker exec spider_dushu cat dushu.log
```

```
docker stop spider_dushu
```

### 3.3 定时器

#### 3.3.1 while循环

#### 3.3.2 crontab

vi /root/runspider.sh

```
#!/bin/bash
  
# cd /root/spider_dushu
# source /root/venvs/dushu/bin/activate
# scrapy crawl guoxue

docker start spider_dushu                      
```

```
chmod +x runspider.sh
```

```
ln -s /root/runspider.sh /usr/bin/run_dushu
```

测试命令

```
run_dushu
```

编辑定时任务，  vi /root/dushu.cron

每半小时（30分）执行一次 run_dushu命令

```
30 * * * * run_dushu
```

格式： 分 时 天 月 周

添加定时任务

```
crontab dushu.cron 
```

```
crontab -l
```

查看定时任务是否添加成功

## 四、中午默写

- ### 在scrapy框架中如何记录日志信息

  ```
  spider.logger.info()/error()/warning()/critical()
  ```

  ```
  LOG_LEVEL = 'INFO'
  LOG_FILE = 'xxx.log'
  ```

- ### 使用ImagesPipeline时需要注意哪些事项

  settings.py

  ```
  IMAGE_STORE=''
  ```

  Spider类的parse()函数中

  ```
  item['image_urls']
  item['images']
  ```

- ### 写出scrapy中哪些地方出现优先级，分别有什么不同

  ```
  # 请求优先级
  yield Request(url, callback, priority=10)
  ```

  ```
  # settings.py
  # 中间件和管道优先级
  ITEM_PIPELINES = {
      'dushu.pipelines.DBPipeline': 300
  }
  
  DOWNLOADER_MIDDLEWARES = {
      'xxx.xxx.LoadDataMiddleware': 100
  }
  ```

  请求优先级数值越高，优先级越高；

  中间件和管道优先级的数值越小，优先级越高。

## 五、作业

- 爬取机票信息
  - 获取所有城市名称与编号
  - 获取近一个月【西安】-【北京】航班信息（班次、时间节点和价格）
- 总结近一周的爬虫知识点
- 重点掌握requests接口请求和docker部署爬虫项目
- 完成`SQL练习题`中10道题。