爬虫第七天

## 一、回顾知识点

### 1.1 日志模块

```
import logging
from logging import StreamHandler, FileHandler
```

- 四个核心部分

  - 日志记录器logger:  记录日志信息
  - 日志处理器 handler：  记录信息之后，由handler去处理
  - 日志过滤器 filter： 对记录信息进行过滤。
  - 日志格式化 formatter： 由处理器对记录的信息按formatter格式进行处理（除HTTPHandler和SMTPHandler之外）。

- 核心方法或函数

  - logging.getLogger(name)  # 默认没有name时，返回root
  - logging.baseConfig() 配置root记录器的格式、处理器等。
  - logging.info()/debug()/warning()/error()/critical() 由root记录器记录日志信息。

- logger记录器的核心方法

  - setLevel(logging.DEBUG|INFO|WARNING|ERROR|FATAL)
  - addHandler(handler)
  - addFilter(Filter)
  - debug()|info()….

- handler处理器的核心方法

  - setLevel(logging.DEBUG|INFO|WARNING|ERROR|FATAL)
  - setFormatter(fmt)

- Formatter初始化参数

  - format 格式化的字符串， 使用`%(日志变量)s` 相关日志变量占位符组成的字符串

    ```python
    'hi, %(name)s, age is %(age)s' % {'age': 20, 'name': 'jack'}
    ```

    ```python
    'hi, %s, age is %s' % ('disen', 30)
    ```

  - datefmt 指定 `%(asctime)s` 日志时间的格式， 通常使用 `%Y-%m-%d %H:%M:%S` 即`年月日 时分秒`格式。

### 1.2 scrapy框架

#### 1.2.1 五大组件两个中间件

- engine  核心引擎
- spider  爬虫类
- scheduler 调度器
- downloader  下载器
- itempipeline  数据管道
- 爬虫中间件、下载中间件

#### 1.2.2 scrapy指令

- scrapy startproject 项目名
- scrapy genspider 爬虫名 域名
- scrapy crawl 爬虫名
  - -o 保存数据到指定的文件中
  - -s 信号（CLOSESPIDER_ITEMCOUNT=30）

- scrapy shell [url]
  - fetch(url)
  - view(response)
  - request:  scrapy.http.Request
  - response:  scrapy.http.Response|HtmlResponse
  - scrapy

#### 1.2.3 Response对象的属性或方法

- body|text|encoding|status|url|request|headers|meta

- xpath()|css() -> scrapy.selector.SelectorList[Selector]

  - extract()
  - get()
  - extract_first()

- css() 中表达式

  - `样式选择器[::text|attr("属性名")]`

- xpath()中表达式

  同lxml的xpath表达式相同。

#### 1.2.4 Request初始化参

- url
- callback 如果未指定，则默认为 `parse`
- priority 优先级的权限值， 值高优先级高
- meta
- headers
- dont_filter 是否过滤重复的url,  True不过滤，Flase过滤.

## 二、scrapy数据管道

### 2.1 指令方式存储

```sh
scrapy crawl 爬虫名 -o xxx.json|csv
```

只适合单页数据爬取，如果多页多层次数据爬取时，不适合此方式。

### 2.2 Item类

作用： 用于区别中哪一页（类型）的数据

用法： 类似于dict用法， 在数据管道类的process_item()方法中，通过isinstance()方法来判断item是哪一类型的。

```python
import scrapy

class BookItem(scrapy.Item):
    book_id = scrapy.Field()
    book_name = scrapy.Field()
    book_cover = scrapy.Field()
    book_url = scrapy.Field()
    author = scrapy.Field()
    tags = scrapy.Field()
    summary = scrapy.Field()


class SegItem(scrapy.Item):
    book_id = scrapy.Field()
    seg_id = scrapy.Field()  # 章节ID
    title = scrapy.Field()
    url = scrapy.Field()


class SegDetailItem(scrapy.Item):
    seg_id = scrapy.Field()  # 章节ID
    content = scrapy.Field()  # 内容
```

### 2.3 Pipeline

- 处理数据的方法

  ```python
  def process_item(self, item, spider):
  	 return item
  ```

  - item参数表示 爬虫类中解释到的数据(yield item)
  - spider参数 表示爬虫类对象
  - 如果item被返回，则表示可以被优先级低的pipeline处理

- 初始化方法

  属于定制方法，可以初始化一些参数或对象，如文件名， 数据库的连接等。

- `process_item`和`init`的调用次数说明

  - `process_item`方法 会被（engine）多次调用
  - `init`随着爬虫程序的启动时创建pipeline类时调用，只会被调用一次

## 三、定量爬虫

### 3.1 基于信号方式 

```
scrapy crawl -s 信号
```

常用的scrapy信号

- **CLOSESPIDER_ITEMCOUNT=**条目的数量
- CLOSESPIDER_PAGECOUNT=请求页的数量
- CLOSESPIDER_ERRORCOUNT=请求错误的数量
- CLOSESPIDER_TIMEOUT=超时的时长

```
scrapy crawl wanben -s CLOSESPIDER_ITEMCOUNT=10
```



## 四、下载中间件

### 4.1 爬虫中间件

```
监测爬虫类与引擎之间的交互数据(请求 request、响应 response、数据item)及异常情况
```

```python
@classmethod
def from_crawler(cls, crawler): pass  # 启动爬虫时用于创建爬虫中间件类的实例对象

def process_spider_input(self, response, spider) # 流程中第6步，engine将请求响应的数据输入给spider时，调用此方法。

def process_spider_output(self, response, result, spider) # 流程中第7步，由spider类解析response数据之后产生结果输出给engine时，调用此方法

def process_spider_exception(self, response, exception, spider): # 解析数据时发异常时
  
def process_start_requests(self, start_requests, spider): # 第一次爬虫发起请求时，调用此方法，即流程中第1步，从Spider->Engine时。
```

### 4.2 下载中间件 [重点]

```
下载中间件是引擎engine和下载器downloader之间的中间件，可以拦截请求和响应以及请求异常的处理。
```

```python
@classmethod
def from_crawler(cls, crawler)

def process_request(self, request, spider)

def process_response(self, request, response, spider)

def process_exception(self, request, exception, spider)
```

- **process_request()**方法可返回的对象（四种可能）
  - scrapy.http.Request
  - scrapy.http.HtmlResponse/Response
  - None 表示不拦截
  - raise IgnoreRequest 不下载这个请求
- **process_response()**方法可以返回的对象
  - scrapy.http.Request 未下载成功请求
  - scrapy.http.Response 进一步封装之后的response

### 4.3 作用

在下载中间件中，可以设置代理、设置cookie、设置请求头以及基于Selenium实现动态js渲染和用户登录。



## 五、规则爬虫 【重点】



## 六、中午默写

- 写出scrapy的工作流程中第3、6两个步骤的描述

  ```
  第3步： 引擎从调度器中获取下载任务， scheduler -> engine
  第6步： 引擎从下载器获取的响应传递给spider, 用于解析。engine-> spider。 ( response.request.callback(response) )
  ```

- 写出logging.baseConfig()方法的参数（4+）

  ```python
  logging.baseConfig(filename,  # 文件处理器参数
                     mode="a",  # 文件处理器参数
                     format,    # formatter
                     datefmt,   # formatter
                     handlers,  # addHandler(),
                     filters,   # addFilter()
                     stream)    # filename设置后, stream无效
  ```

  

- 根据左边AA表结构和右边的查询结果，写出查询的SQL

  ```
  ---------------------------        ----------------------
  | year  |  month | amount |	       | year  |  m1 |   m2 |
  ---------------------------        ----------------------
  | 1991  |    1   | 1.1    |        | 1991  | 1.1 | 1.2  |
  ---------------------------        ----------------------
  | 1991  |    2   | 1.2    |        | 1992  | 2.1 | 2.2  |
  ---------------------------        ----------------------
  | 1992  |    1   | 2.1    |
  ---------------------------
  | 1992  |    2   | 2.2    |
  ---------------------------
  ```

  - join连接表方式

    ```sql
    select a1.year, a1.amount as m1, a2.amount as m2
    from AA a1
    join AA a2 on (a1.year = a2.year)
    where a1.month=1
    and a2.month=2;
    ```

  - if/case判断函数方式

    ```
    select if(条件， 成立的结果， 不成立的结果);
    ```

    ```sql
    select year, 
    			max(round(if(month=1, amount, 0),1)) m1, 
    			max(round(if(month=2, amount, 0),1)) m2
    from AA
    group by year;
    ```

    ```sql
    select year, 
           max(round(m1, 1)) as m1,
           max(round(m2, 1)) as m2
    from (
        select year,
               case when month=1 then amount else 0 end as m1,
               case when month=2 then amount else 0 end as m2
        from AA
    ) a
    group by year;
    ```

    ```sql
    select year, 
           max(round(m1, 1)) as m1,
           max(round(m2, 1)) as m2
    from (
        select year,
               case month when 1 then amount else 0 end as m1,
               case month when 2 then amount else 0 end as m2
        from AA
    ) a
    group by year;
    ```

    

## 七、作业

- 复习mysql

  - DML/DDL/DTL
  - 视图、索引
  - 常见函数

- 企查查的数据爬虫(规则爬虫方式)-练习

  https://www.qichacha.com/g_SAX.html

  **去重解决办法**：

  在**爬虫中间件**中，打开爬虫的方法中，通过spider对象增加urls集合

  ```python
  def spider_opened(self, spider):
      spider.urls = set()
  ```

  在**下载中间件**的process_request()方法内，判断请求的url是否已存在

  ```python
  def process_request(self, request, spider):
      if request.url in spider.urls:
         raise IgnoreRequest()  # 不下载重复的
  ```

  在**下载中间件**的process_response()方法中，如果response成功，则将请求的url存放到urls中

  ```python
  def process_response(self, request, response, spider):
      if response.status == 200:
         spider.urls.add(request.url)
  ```

- 读书网的规则爬虫(所有分类的所有图书)

  https://www.dushu.com/book/

- 大众点评中`古晋食府`的所有评论(Cookie池)

  http://www.dianping.com/shop/38079741/review_all

  字体加密解决办法：

  https://blog.csdn.net/sinat_32651363/article/details/85123876

